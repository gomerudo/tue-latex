\section{Related work}\label{sec:related}

Neural Architecture Search (NAS) is a broad field that can be approached from different perspectives, however we only study the area using reinforcement learning for the search of Convolutional Neural Networks (CNNs). For a cleaner presentation, in section~\ref{sec:related:summary} we sum up the relevant projects to our research, highlighting seven aspects per work: the search space explored, the performance estimation strategy, the policy optimization algorithm applied, the input datasets, the computational resources per experiment, whether or not the transferability of the discovered architecture was tested, and the main claims with respect to the numeric performance of the results. Next, in section~\ref{sec:related:analysis} we discuss the main contributions of those works and their differences, pointing to the features that inspire our research.

\subsection{Summary}\label{sec:related:summary}

We consider seven main investigations, which have contributed differently to the field of NAS via reinforcement learning\footnote{Some of these works address the generation of both Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). However, we omit the details for RNNs since they are out of the scope of this investigation.}. For their presentation we sort them in chronological order, and feature them by the seven attributes listed above.

\textbf{MetaQNN}~\citep{BakerNAS}. \emph{Search space}: chain-structured neural networks composed of convolutions, pooling operations and fully-connected layers. \emph{Performance estimation strategy}: test accuracy after 20 epochs of training. \emph{Policy optimization}: \textsc{Q-learning} with $\epsilon$-greedy exploration and memory replay. \emph{Input datasets}: Street View House Numbers (SVHN), CIFAR-10 and MNIST. \emph{Resources}: 10 GPUs running for 8-10 days per dataset. \emph{Transferability}: test the architecture generated for CIFAR-10 on the remaining two datasets with successful results. \emph{Performance}: This technique yielded to architectures that outperform other chain-structured networks and remain competitive against more complex architectures when considering the test error rate.


\textbf{NAS}~\citep{ZophNAS1}. \emph{Search space}: multi-branch CNNs that contain convolutions, rectified linear units and batch normalization. \emph{Performance estimation strategy}: test accuracy after 50 epochs. \emph{Policy optimization}: \textsc{Reinforce}, with a policy modeled as a Recurrent Neural Network (RNN). \emph{Input datasets}: CIFAR-10. \emph{Resources}: 800 GPUs running for 28 days. \emph{Transferability}: No studied. \emph{Performance}: The obtained architecture proved competitive against state-of-the-art models with respect to the test accuracy, and outperform them in terms of the test error rate and training speed.


\textbf{NASNet}~\citep{ZophNAS2}. \emph{Search space}: stacked multi-branch convolutional \textit{cells} containing convolutions, additions, concatenations, pooling and identities. \emph{Performance estimation strategy}: test accuracy after 50 epochs. \emph{Policy optimization}: \textsc{Reinforce}, with a policy modeled as a RNN. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 500GPUs running for 4 days. \emph{Transferability}: the discovered cell is tested on the ImageNet dataset. \emph{Performance}: the results outperform state-of-the-art architectures for both accuracy and test error rate for CIFAR-10, and remain competitive for ImageNet with minor tuning.



\textbf{BlockQNN-V1}~\citep{BlockQNN}. \emph{Search space}: stacked multi-branch convolutional \textit{cells} composed of convolutions, additions, concatenations, pooling and identities. \emph{Performance estimation strategy}: test accuracy after 12 epochs only, with a penalization of the network's density and the number of Floating Point Operations (FLOPs). \emph{Policy optimization}: \textsc{Q-learning} with $\epsilon$-greedy exploration. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 32 GPUs running for 3 days. \emph{Transferability}: the discovered cell is tested on ImageNet. \emph{Performance}: competitive results on CIFAR-10 regarding the test error rate and \textit{very} competitive on ImageNet with minor tuning.
 
\textbf{BlockQNN-V2}~\citep{BlockQNN}. \emph{Search space}: stacked multi-branch convolutional \textit{cells} composed of convolutions, additions, concatenations, pooling and identities. \emph{Performance estimation strategy}: pre-trained neural network that estimates the accuracy of any network based on its complexity. \emph{Policy optimization}: \textsc{Q-learning} with $\epsilon$-greedy exploration. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 1 GPU running for 20 hours \emph{Transferability}: the discovered cell is tested on ImageNet. \emph{Performance}: lower but yet competitive results with respect to BlockQNN-V1.

\textbf{Path-level Network Transformation}~\citep{PathNAS}. \emph{Search space}: morphisms of a pre-defined network via Net2Net-like transformations on convolutions and pooling layers. \emph{Performance estimation strategy}: test accuracy after 20 epochs with re-usage of weights of the network before transformation. \emph{Policy optimization}: \textsc{Reinforce} with policy represented by a tree-structured LSTM. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 200 GPU hours. \emph{Transferability}: the discovered network is tested on ImageNet. \emph{Performance}: excel state-of-the-art human-designed and NAS-designed networks on both datasets.

\textbf{ENAS}~\citep{ENAS}. \emph{Search space}: all sub-graphs from a fully-connected Directed Acyclic Graph (DAG) of 7 nodes, where each node can be a convolution, depth-wise separable convolution, max pooling, and average pooling. \emph{Performance estimation strategy}: test accuracy after 150 epochs of training. \emph{Policy optimization}: \textsc{Reinforce} with a policy represented by a RNN. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 1 GPU running for 7 hours. \emph{Transferability}: not studied. \emph{Performance}: the discovered network performed almost as good as the NASNet~\citep{ZophNAS2} in terms of the test error (0.24\% difference only).


\subsection{Analysis}\label{sec:related:analysis}

The works above display the complexity of Neural Architecture Search (NAS), in the sense that  This is, while one would hope to solve the problem as a whole, there is a number of factors that impede to satisfy this ambition, forcing researchers to settle for particular objectives.

Perhaps the biggest of those factors is the cost of training neural networks. In a typical reinforcement learning setting, thousands of steps are performed to allow the agent to learn a policy based on rewards from its interaction with the environment at each step. For NAS, this reward is, in principle, the accuracy after training on a test set for each of the sampled networks, meaning that thousands of train-evaluation procedures are performed per experiment. An example of the expensiveness of this approach is the long training reported by~\citet{ZophNAS1}, where the computational cost is beyond the possibilities of many researches: hundreds of GPUs running during almost a month. In an attempt to avoid the bottleneck caused by this costly procedure, different relaxations are proposed to estimate the performance of the networks, rather than performing expensive training procedures: reduce the number of epochs~\citep{BakerNAS,BlockQNNprev}, constrain the search space to generate similar networks that can re-use weights and allow for a shorter training~\citep{PathNAS}, or skip the training of the networks by predicting their performance~\citep{BlockQNN}. Perhaps appealing at first, these relaxations might cause an incorrect assessment of the real performance of the architectures, favouring the ones that might not be as good as the estimation suggests. This observation was considered in the BlockQNN project~\citep{BlockQNN}, where the reward function takes the accuracy after training with early-stop but penalizes it with the network's density and FLOPs, making the reward more relevant to the final performance of the architectures.

A second factor influencing the performance of the NAS strategies is the definition of the search space. In that regard, some researchers opt for a clearly diverse space~\citep{ZophNAS1,ZophNAS2,BlockQNN} and others for a more compact one~\citep{PathNAS,ENAS}. We make the observation that the relaxations on the search space seem highly related to the computational resources required, where a less restricted space usually requires more computational power. Concrete examples of this observation are the NAS~\citep{ZophNAS1} and NASNet~\citep{ZophNAS2} projects, where the drastic difference in the computational cost is claimed to be caused by the reduction of the search space in NASNet, since the same reinforcement learning setting is applied for both of the experiments. Another work that supports our observation is MetaQNN~\citep{BakerNAS}, where the search space is restricted to chain-structured networks avoiding the complexity of a multi-branch space and resulting in a more feasible experiment with respect to the number of GPUs and days required per experiment, even though the cost of the training-evaluation procedure per network is still considerable. Lastly, ENAS~\citep{ENAS} shows that a more drastic reduction of the search space leads to an impressively shorter duration of the experiments with only 1 GPU. ENAS designs a search space modeled as a super graph where the sampled architectures are sub-graphs only. In fact, the design of this search space has been shown to be the primary cause of the performance of the method and not the the reinforcement learning procedure~\citep{ENASbad}, meaning that the space itself contains well-performing networks that could be explored with search strategies other than reinforcement learning and yet leading to similar performance.

From the analysis above we make two notes. Firstly, it can be seen that there exists a trade-off between the search space and the performance estimation strategy used in the experiments. This is, when defining a more compact search space there is more chance to use the actual accuracy after a considerable number of training epochs, whereas when considering a more complex one few epochs are preferred to avoid the bottleneck of the train-evaluation procedures. Secondly, the definition of the search space constrains the ultimate goal of the policy that is to be learned, i.e. a less restricted space usually targets for a policy that is optimized to deal with all aspects of the design of the architectures, and a reduced one serves only as a simple search method that can potentially be replaced, loosing the conceptual benefits of reinforcement learning. %Based on this, we consider that among all the studied works BlockQNN-V1~\citep{BlockQNN} provides the best framework for NAS via reinforcement learning, since it provides a complex search space and a performance estimation strategy that takes in consideration the real performance of the network and reduces the bias of the reward.

%Other recent approaches that have been applied to NAS are those of~\citet{DARTS} and \cite{ProgressiveNAS}.