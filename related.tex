\section{Related work}\label{sec:related}

% TODO: changes in progress for this section.. 

\subsection{Reinforcement learning}\label{sec:related:rl}

The key to reinforcement learning is the algorithm used to optimize the \textit{policy} $\pi_\theta$ (see Definition~\ref{def:preliminaries:rl}). Through the years, researchers have proposed different algorithms, such as \textsc{Reinforce}~\citep{Reinforce}, Q-learning~\citep{Qlearning}, Actor-Critic~\citep{ActorCritic}, Deep-Q-Network (DQN)~\citep{DQN}, Trust Region Policy Optimization (TRPO)~\citep{TRPO}, and Proximal Policy Optimization (PPO)~\citep{PPO}. These algorithms have successfully solved problems in a variety of fields, from robotics~\citep{RLRobotics} and video games~\citep{MonteCarlo, DQN} to traffic-control~\citep{RLTraffic} or computational resources management~\citep{RLresourcemanagement}, showing the power and utility of the reinforcement learning framework. Despite its success, a theoretical flaw of RL is that the policy learned only captures the one-to-one \textit{state-action} relation of the environment in question, making it necessary to perform individual runs for every new environment of interest. The latter is a costly trait of this standard form of RL since it typically requires thousands of steps to converge to an optimal policy.


A research area that addresses this problem is meta-RL, in which agents are trained to learn transferable policies that do not require training from scratch on new problems. We identify two types of meta-RL algorithms: the ones that learn a \textit{good} initialization for the neural networks representing the policy\footnote{The term \textit{deep} meta-reinforcement learning comes from the usage of \textit{deep} models, such as neural networks, to represent the policy to be learned.}, and the ones that learn policies that can adapt their decision-making strategy to new environments, ideally without further training. First, Model Agnostic Meta-Learning (MAML)~\citep{MAML} learns an initialization that allows few-shot learning in RL; however, it relies on the strong assumption of working with a distribution of similar environments, which cannot always be guaranteed, thus limiting its scope. Second, two algorithms have been proposed to learn policies that, once deployed, can adapt their decision-making strategy to new environments, ideally without further training: \textit{Learning to reinforcement learn}~\citep{LtRL} and RL$^2$~\citep{RL2}. These methods aim to learn a more sophisticated policy modeled by a Recurrent Neural Network (RNN) that captures the relation between states, actions, and meta-data of past actions. What emerges is a policy that can adapt its strategy to different environments. The main difference between the two works is the set of environments considered: for ~\citet{LtRL} they come from a parameterized distribution, whereas for~\citet{RL2} they are relatively unrelated. The latter is an essential advantage over MAML, making them more suitable for scenarios where a distribution of environments cannot be guaranteed. Third, Simple Neural AttentIve Learner (SNAIL) extends the idea behind \textit{Learning to reinforcement learn} and RL$^2$ by using a more powerful temporal-focused model than the simple RNN. We note that none of these approaches have been applied to NAS.


\subsection{Neural Architecture Search}\label{sec:related:nas}

As introduced earlier in Section~\ref{sec:preliminaries:nas}, it is possible to address Neural Architecture Search (NAS) in different ways. Remarkable results have been achieved by applying optimization techniques such as Bayesian optimization, evolutionary algorithms, gradient-based search, and reinforcement learning. We are interested in reinforcement learning for NAS, due to the variety of works that have achieved state-of-the-art results.
%For the sake of completeness, in Appendix~\ref{app:related}, we present a concise and systematic survey of the relevant works discussed here.
For other work in NAS, we refer to the survey of~\citet{NASsurvey}.

%TODO: add citations.

Although the ultimate goal of NAS is to come up with a straightforward fully-automated solution that can deliver a neural architecture for a machine learning task on any dataset of interest, there exist several factors that impede that ambition. Perhaps the most important of these factors is the high computational cost of NAS with reinforcement learning, which imposes constraints on different elements that impact the scope of the solutions. The first bottleneck is the computation of the reward, which typically is the test accuracy of the sampled architectures after training. Because of the expensiveness of such computation,  researchers have proposed various \textit{performance estimation strategies} to avoid expensive training procedures, and they have also imposed some constraints over the \textit{search space} considered so that a lower number of architectures get sampled and evaluated. For the first aspect, we observe several relaxations: reducing the number of training epochs~\citep{BakerNAS,BlockQNN}, sharing of weights between similar networks~\citep{PathNAS, ENAS}, or entirely skipping the train-evaluation procedure by predicting the performance of the architectures~\citep{BlockQNN}. Although these alternatives have successfully reduced the computation time, they pay little attention to the effect of their potentially unfair estimations on the decision-making of the agent, and therefore, one should treat them carefully. On the other hand, for the \textit{search space}, crucial choices are the cardinality of the space and the complexity of the architectures. Some researchers opt for ample spaces with various types of layers and no restrictions in their connections~\citep{ZophNAS1,ZophNAS2,BlockQNN}, whereas others prefer them smaller, such as a chain-structured space~\citep{BakerNAS}, or a multi-branch space modeled as a fully-connected graph with low cardinality~\citep{ENAS}.

It is important to note that an approach can dramatically reduce its computation time with relaxations on the search space alone. For instance, the same methodology can decrease its computational cost by a factor of 7 (28 days to 4 days, with hundreds of GPUs in both cases) if the space is restricted in the types of layers and the number of elements allowed in the architectures~\citep{ZophNAS1, ZophNAS2}. Furthermore, a constrained search space used jointly with some performance estimation strategy can reduce the cost to only 1 day with 1 GPU such as in BlockQNN-V2~\citep{BlockQNN} and ENAS~\citep{ENAS}; however, this drastic reduction of the computational time should be treated with caution. In the case of BlockQNN-V2, the estimation of the performance of the networks (i.e., accuracy at a given epoch) depends on a surrogate prediction model that is not studied in detail by the authors, thus leaving room for potentially wrong predictions. On the other hand, a recent investigation~\citep{ENASbad} shows that the quality of the networks delivered by ENAS is not a consequence of reinforcement learning, but of the search space, which contains a majority of well-performing architectures that can be explored with a less expensive procedure such as random search, therefore losing its character of \textit{artificially smart} search.

% It becomes clear how the constraints on the \textit{search space} and the \textit{performance estimation strategy} can impact the robustness of the solutions, but 
Another factor impacting a NAS with reinforcement learning work is the \textit{input dataset} used. Although they usually transfer the best CIFAR-based architecture designed by the agent to the ImageNet dataset~\citep{BakerNAS, ZophNAS1, ZophNAS2, PathNAS, ENAS}, none of them make the agent design networks for other datasets. Furthermore, none of the works give insight on how using a different dataset could affect the complexity of the search. We believe that the lack of study for other datasets is ascribed to the costly task-oriented design of the reinforcement learning algorithms used, \textsc{Q-learning} and \textsc{Reinforce}, that requires to train the agent from scratch for every environment (i.e., a dataset) of interest. The authors do not justify the choice of these algorithms; hence, it would be desirable to study other reinforcement learning algorithms in the same NAS scenarios.