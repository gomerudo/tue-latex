\section{A concise survey of Neural Architecture Search with reinforcement learning}\label{app:related}

When addressing Neural Architecture Search (NAS) with reinforcement learning the main interest of the current research is usually that of obtaining an architecture that achieves state-of-the-art \textit{performance} on baseline datasets however, other aspects are also relevant in a NAS implementation. In this section we systematically survey the existing reinforcement learning works using seven variables that affect their utility: the \textit{search space}, the \textit{performance estimation strategy}, the \textit{reinforcement learning algorithm}, the \textit{input datasets}, the computational \textit{resources} demanded, the \textit{transferability properties} of the methodologies, and the \textit{performance} of the discovered architectures. For the sake of conciseness we omit other strategies for NAS, but we refer the reader to the survey of~\citet{NASsurvey} where approaches such as evolutionary algorithms or gradient descent are studied.

% Because of the huge diversity of NAS with reinforcement learning, in Section~\ref{sec:related:summary} we summarize the relevant works featured by the aforementioned variables, and in Section~\ref{sec:related:analysis} we present a global analysis of them.


% \subsubsection{Summary of works}\label{sec:related:summary}

\hspace{\parindent} \textbf{MetaQNN}~\citep{BakerNAS}. \emph{Search space}: chain-structured neural networks composed of convolutions, pooling operations and fully-connected layers. \emph{Performance estimation strategy}: test accuracy after 20 epochs of training. \emph{RL algorithm}: \textsc{Q-learning} with $\epsilon$-greedy exploration and experience replay. \emph{Input datasets}: Street View House Numbers (SVHN), CIFAR-10 and MNIST. \emph{Resources}: 10 GPUs running for 8-10 days per dataset. \emph{Transferability}: the architecture generated for CIFAR-10 was reused on the remaining two datasets with successful results. \emph{Performance}: This technique yielded architectures that outperform other chain-structured networks and remain competitive against more complex architectures when considering the test error rate.


\textbf{NAS-RL}~\citep{ZophNAS1}. \emph{Search space}: multi-branch CNNs that contain convolutions, rectified linear units and batch normalization. \emph{Performance estimation strategy}: test accuracy after 50 epochs. \emph{RL algorithm}: \textsc{Reinforce}, with a policy modeled as a Recurrent Neural Network (RNN). \emph{Input datasets}: CIFAR-10. \emph{Resources}: 800 GPUs running for 28 days. \emph{Transferability}: Not studied. \emph{Performance}: The obtained architecture outperformed state-of-the-art models with respect to the test accuracy and training speed.

\textbf{NASNet}~\citep{ZophNAS2}. \emph{Search space}: stacked multi-branch convolutional \textit{cells} containing convolutions, additions, concatenations, pooling and identities. \emph{Performance estimation strategy}: test accuracy after 50 epochs. \emph{RL algorithm}: \textsc{Reinforce}, with a policy modeled as a RNN. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 500GPUs running for 4 days. \emph{Transferability}: the discovered cell is tested on the ImageNet dataset. \emph{Performance}: the results outperform state-of-the-art architectures for CIFAR-10 and remain competitive for ImageNet with minor tuning.

\textbf{BlockQNN-V1}~\citep{BlockQNN}. \emph{Search space}: stacked multi-branch convolutional \textit{cells} composed of convolutions, additions, concatenations, pooling and identities. \emph{Performance estimation strategy}: test accuracy after 12 epochs only, with a penalization of the network's graph density and the number of Floating Point Operations (FLOPs). \emph{RL algorithm}: \textsc{Q-learning} with $\epsilon$-greedy exploration and experience replay. \emph{Input datasets}: CIFAR-100. \emph{Resources}: 32 GPUs running for 3 days. \emph{Transferability}: the discovered cell is tested on CIFAR-10 and ImageNet. \emph{Performance}: competitive results on CIFAR-10 regarding the test error rate and \textit{very} competitive on ImageNet with minor tuning.
 
\textbf{BlockQNN-V2}~\citep{BlockQNN}. \emph{Performance estimation strategy}: pre-trained neural network that estimates the accuracy of any network based on its complexity. \emph{Resources}: 1 GPU running for 20 hours. \emph{All other aspects}: same as BlockQNN-V1. \emph{Performance}: higher test error rates but yet competitive with respect to BlockQNN-V1.

\textbf{Path-level Network Transformation}~\citep{PathNAS}. \emph{Search space}: morphisms of a pre-defined network via Net2Net-like~\citep{Net2Net} transformations on convolutions and pooling layers. \emph{Performance estimation strategy}: test accuracy after 20 epochs with re-usage of the network's weights before the last transformation. \emph{RL algorithm}: \textsc{Reinforce} with policy represented by a tree-structured LSTM~\citep{TreeLSTM}. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 200 GPU hours. \emph{Transferability}: the discovered network is tested on ImageNet. \emph{Performance}: excel state-of-the-art human-designed and NAS-designed networks on both datasets.

\textbf{ENAS}~\citep{ENAS}. \emph{Search space}: all sub-graphs from a fully-connected Directed Acyclic Graph (DAG) of 7 nodes, where each node can be a convolution, depth-wise separable convolution, max pooling, and average pooling. \emph{Performance estimation strategy}: test accuracy after 150 epochs of training, with shared weights between parent and children graphs. \emph{RL algorithm}: \textsc{Reinforce} with a policy represented by a RNN. \emph{Input datasets}: CIFAR-10. \emph{Resources}: 1 GPU running for 7 hours. \emph{Transferability}: not studied. \emph{Performance}: the discovered network performed almost as good as the NASNet~\citep{ZophNAS2} in terms of the test error (0.24\% difference only).


% \subsubsection{Analysis}\label{sec:related:analysis}
