\section{Methodology}\label{sec:methodology}

The methodology that we set for this work can be split in two: the Neural Architecture Search (NAS) artifacts and the reinforcement learning framework. 
%In this section we elaborate on each of these elements and we present the setting of our experiments.
In a nutshell, we make use of the deep-meta reinforcement learning framework proposed by~\citet{LtRL} and~\citet{RL2} to learn from a distribution of Markov Decision Processes\footnote{In this report, we use the terms MDPs and environments as equivalents.} (MDPs), rather than from a single MDP as it is done in a standard reinforcement learning setting. The MDPs that we consider are instances of the meta-dataset collection assembled by~\citet{MetaDataset}, which we use for standard classification tasks. With respect to the NAS elements, we designed a search space similar to the one in BlockQNN~\citep{BlockQNN} and we apply the same performance strategy they proposed. Under this setting we set two main experiments: a) generate both chain-structured and multi-branch networks, for which we test their transferability on a new dataset, and b) test the transferability of the learned policy to solve a new MDP (i.e. an image classification task on a new dataset). We compare our results with those generated using standard Q-learning with the same search space and performance estimation strategies, but using CIFAR-10 as the only MDP.

In the remaining of this section we deepen into these components and present the setting for our experiments.

\subsection{The Neural Architecture Search elements}\label{sec:methodology:nas}

As it was sketched out by~\cite{NASsurvey}, Neural Architecture Search (NAS) can be studied from three different flanks: the search strategy, the search space, and the performance estimation strategy. In this section we elaborate on the latter two, which follow the base methodology of BlockQNN~\citep{BlockQNN} with some minor modifications.

\subsubsection{The search space}\label{sec:methodology:ss}

Our search space is highly inspired by the BlockQNN project~\citep{BlockQNN}, which defines it as all possible architectures that can be generated by sequentially stacking elements from a so-called Network Structure Code (NSC) space that is described in Table~\ref{tab:nas:nsc}. Originally, the NSC space contains 7 elements, each with a pre-defined range for an associated hyperparameter and the allowed incoming connections (i.e. the inputs), nevertheless we ignore the \textit{identity} operation, since we consider it redundant under our setting.

In principle, this search space does not impose any constraint upon the allowed connections in the architecture as long as they are valid, making it possible to explore both multi-branch and chained structures; however, to make the search more efficient we introduce minor restrictions depending on the type of structure that is considered. In the case of the chained-structured networks we ignore the \textit{concatenation} and \textit{addition} operations, since only one predecessor is considered to build the chain. On the other hand, to avoid high-dimensional blocks that can cause memory issues for the multi-branch structures, we do not consider the \textit{concatenation} operation as a possible action during the design of the networks, but instead we use it as a merge operation when the resulting network has more than one leaf.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[ht]
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
\textbf{Name}   & \textbf{Type} & \textbf{Kernel size} & \textbf{Predecessor 1} & \textbf{Predecessor 2} \\ \midrule
Convolution     & 1             & \{1, 3, 5\}          & \textbf{K}             & $\emptyset$            \\
Max Pooling     & 2             & \{1, 3\}             & \textbf{K}             & $\emptyset$            \\
Average Pooling & 3             & \{1, 3\}             & \textbf{K}             & $\emptyset$            \\
Addition        & 5             & $\emptyset$          & \textbf{K}             & \textbf{K}             \\
Concatenation   & 6             & $\emptyset$          & \textbf{K}             & \textbf{K}             \\
Terminal        & 7             & $\emptyset$          & $\emptyset$            & $\emptyset$            \\ \bottomrule
\end{tabular}
\caption{The search space presented in Neural Structure Code (NSC) format, as in the original BlockQNN paper~\citep{BlockQNN}. On purpose, the \textbf{Type} 4 code is missing since we preserved the original enumeration. The values in the set \textbf{K} are dependent on the level of the network, i.e. $\textbf{K} = \{1, 2, \dots , \text{current layer index} - 1 \}$ }
\label{tab:nas:nsc}
\end{table}

We note that this space is flexible to search for complete architectures or \textit{cells} only. For our experiments, we take advantage of this as it is presented in section~\ref{sec:methodology:experiments}.

\subsubsection{The performance estimation strategy}\label{sec:methodology:pss}

We use a penalized early-stop accuracy as the one in BlockQNN~\citep{BlockQNN}, as described in equation~\ref{eq:pes}, where $\mu, \rho \in [0, 1]$ are weights for the penalization terms, FLOPs is the number of floating point operations of the block, and Density is the edge number divided by the dot number when treating the network as a Directed Acyclic Graph (DAG). 

We prefer this strategy over others found in literature because of our interest in training the network with few epochs to save computing time during the learning process. Furthermore, this penalized version has been shown useful to compensate the pitfall of early-stop that may underestimate good architectures. For a deeper study, we refer to the original paper.

\begin{equation}\label{eq:pes}
reward = \text{ACC}_\text{EarlyStop} - \mu \log(\text{FLOPs}) - \rho \log(\text{Density})
\end{equation}

\subsection{The reinforcement learning framework}\label{sec:methodology:rl}

%by interacting with the environment a so-called \textit{agent} learns the distribution of state-actions pairs over the \textit{reward} domain, allowing to accomplish a goal on the environment in a guided way. In other words, the agent is able to capture the convenience of the actions at specific states, so that it can efficiently achieve its goal.

It has already been stressed that our research uses reinforcement learning as the search strategy to address the NAS problem. 
%The choice is not arbitrary, and we base it on the theoretical advantages of reinforcement learning over other goal-directed learning approaches~\citep{RLIntroBook}. 
Particularly, we are interested in exploiting the framework's foundation: the distribution of \textit{state}-\textit{action} pairs over the \textit{reward} domain that is learned from the interaction of the \textit{agent} with the \textit{environment}. We aim to strengthen that distribution by making use of a deep-meta-reinforcement learning framework~\citep{LtRL, RL2}, which is different from the standard reinforcement learning in two main aspects: 1) the agent is challenged to face more than one environment, and 2) the distribution is now dependant on the whole history of state, actions and rewards; instead of the simple state-action pairs. The distribution is learned with the help of a \textit{policy} modeled with a Recurrent Neural Network (RNN) that thanks to its internal dynamics and the learning setting, is able to adapt to different environments and achieve its goal efficiently.

When applied to NAS this extension has two main implications. The first one is that by letting the agent learn from different environments (i.e. datasets), the resulting architecture after training is built based on a more complex distribution that takes into account the current environment and the ``long-term" effect of every executed action (change in the architecture), leading to a better design of the architecture. The second implication is that the resulting policy can be trained on a previously unseen environment (i.e. a new dataset) to design an \textit{ad hoc} architecture in a faster way, since the policy already has a ``general" distribution embedded in the RNN, and therefore the new training will only integrate the information specific to the new environment.

Here below, we formally present the version of deep-meta reinforcement learning that we implement, which is primarily based on the methodology of~\citet{LtRL}, with a small modification taken from the parallel and complementary research of~\citet{RL2}.

\subsubsection{Formalization}\label{sec:methodology:rl:formal}

Let $\mathcal{D}$ be a distribution over a set of Markov Decision Processes (MDPs). An appropriately structured agent, embedding a recurrent neural network, is trained by interacting with a sequence of MDP environments (also called \textit{tasks} or \textit{trials}) through episodes. At the start of a new task $m \sim \mathcal{D}$ an initial state for this task is sampled, and the internal state of the agent (i.e., the pattern of activation over its recurrent units) is reset. The agent then starts a number $n$ of episodes for which it executes its action-selection strategy for a certain number of discrete time-steps. At each step $t$ an action $a_t \in A$ is executed as a function of the whole history $H_t = \{x_0, a_0, r_0, . . . , x_{t-1}, a_{t-1}, r_{t-1}, x_t\}$ of the agent interacting in the MDP $m$ during the current episode (set of states $\{x_s\}_{0 \leq s \leq t}$, actions $\{a_s\}_{0 \leq s < t}$, and rewards $\{r_s\}_{0 \leq s < t}$ observed since the beginning of the first episode, when the recurrent unit was reset). The network weights are trained to maximize the sum of observed rewards over all steps and episodes.

After training, the agent's policy is fixed (i.e. the weights are frozen, but the activations are changing due to input from the environment and the hidden state of the recurrent layer), and it is evaluated on a set of MDPs that are drawn either from the same distribution $\mathcal{D}$ or slight modifications of that distribution (to test the generalization capacity of the agent). The internal state is reset at the beginning of the evaluation of any new task.

% \subsubsection{Specifics}

\subsubsection{The action space}\label{sec:methodology:rl:as}

Contrarily to the works studied in section~\ref{sec:related}, we formulate the action space as a discrete space similar to those of the Atari environments. Table~\ref{tab:rl:as} shows this space in detail. 

In particular, given the maximum number $E$ of elements in the network, we model the space as an matrix of shape $E \times 5$, where the 5 is length of the NSC presented in section~\ref{sec:methodology:ss}. In this representation, we allow the agent to control a pointer that can be shifted upwards or downwards to select the predecessor. As a consequence, the agent is able to learn the best way to move this pointer to increase its accumulated reward during the trial.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[ht]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Structure type}                                         & \textbf{Encoding}                            & \textbf{Description}                                                                                                                            \\ \midrule
\begin{tabular}[c]{@{}c@{}}Chained,\\ Multi-branch\end{tabular} & convolution\_k-\{1, 3, 5\}\_pred1-L\_pred2-0 & \begin{tabular}[c]{@{}c@{}}Convolution with a kernel in\\ set \{1, 3, 5\}. Predecessor1 is\\ always the last layer (L).\end{tabular}            \\
\begin{tabular}[c]{@{}c@{}}Chained,\\ Multi-branch\end{tabular} & maxpooling\_k-\{1, 3\}\_pred1-L\_pred2-0     & \begin{tabular}[c]{@{}c@{}}Max-pooling with a pool size\\ in set \{1, 3\}. Predecessor1 is\\ always the last layer (L).\end{tabular}            \\
\begin{tabular}[c]{@{}c@{}}Chained,\\ Multi-branch\end{tabular} & avgpooling\_k-\{1, 3\}\_pred1-L\_pred2-0     & \begin{tabular}[c]{@{}c@{}}Average-pooling with a pool\\ size in set \{1, 3\}. Predecessor1\\ is always the last layer (L).\end{tabular}        \\
\begin{tabular}[c]{@{}c@{}}Chained,\\ Multi-branch\end{tabular} & terminal\_k-0\_pred1-0\_pred2-0              & The terminal state.                                                                                                                             \\
Multi-branch                                                    & add\_k-0\_pred1-L\_pred2-BL                  & \begin{tabular}[c]{@{}c@{}}Addition with Predecessor1\\ and Predecessor2 controlled\\ by an internal pointer in the\\ environment.\end{tabular} \\
Multi-branch                                                    & predshift\_p-\{1, 2\}\_op-\{U, D\}           & \begin{tabular}[c]{@{}c@{}}Move the predecessors (1 or 2)\\ upwards (U) or downwards (D).\end{tabular}                                          \\ \bottomrule
\end{tabular}
\caption{caption}
\label{tab:rl:as}
\end{table}

\subsubsection{The environments}\label{sec:methodology:rl:datasets}

In section~\ref{sec:methodology:rl:formal} there is an assumption of working with a distribution $\mathcal{D}$ of MDPs from which different environments are sampled. In our case, this distribution is composed of of image datasets from the meta-dataset collection~\citep{MetaDataset} that is originally composed of 10 datasets that we reduce to 9. Originally, the meta-dataset is intended to improve the performance of \textit{meta} models for few-shot classification and therefore, we perform some modifications for our tasks that are standard classification tasks.

In specific, we define a different train-validation usage of the datasets and we set an order in the usage of the environments for the different trials needed for deep meta-reinforcement learning. The order imposed is based on the complexity of the classification tasks, where we consider simpler tasks the ones with few classes and relatively few observations, so that the agent can gradually learn to behave on different environments. We consider this order important to facilitate the learning curve of the agent, since intuitively complex datasets could lead to low rewards related to the complexity of the task more than to the effectiveness of the actions performed. Table~\ref{tab:meta-dataset} shows our usage of the meta-dataset.

We consider important to note that in their plain versions, these datasets have different shapes (width and height). To solve this, we stick to the original meta-dataset pre-processing that resizes the images to a shape of $84 \times 84$ with the 3 RGB channels using a bilinear interpolation, so that all images in the super collection share the same dimensions.

\begin{table}[ht]
\centering
\begin{tabular}{@{}cccccc@{}}
\toprule
Dataset ID    & Dataset name                              & Usage      & Trial        & N classes & N observations \\ \midrule
aircraft      & FGVC-Aircraft                             & Train      & 3            & 100       & 10000          \\
cu\_birds     & CUB-200-2011                              & Train      & 5            & 200       & 11788          \\
dtd           & Describable Textures                      & Train      & 2            & 47        & 5640           \\
fungi         & FGVCx Fungi                               & Train      & 7            & 1394      & 89760          \\
omniglot      & Omniglot                                  & Validation & -            & 1623      & 32460          \\
quickdraw     & Quick, Draw!                              & Train      & 6            & 345       & 50426266       \\
traffic\_sign & German Traffic Sign                       & Train      & 1            & 43        & 39209          \\
vgg\_flower   & VGG Flower                                & Train      & 4            & 102       & 8189           \\
ilsvrc\_2012  & ImageNet                                  & Validation & -            &           &                \\ \bottomrule
\end{tabular}
\caption{Subset of the meta-dataset~\citep{MetaDataset} that we use for the experiments. Three main things can be noted: a) the MSCOCO dataset was removed, b) the datasets for train and validation were modified with respect to the original work, and c) an order in the trials was established to allow the agent to gradually face more complex datasets.}
\label{tab:meta-dataset}
\end{table}

% TODO: explain the 

\subsection{Software specifics}\label{sec:methodology:software}

In an effort to make our research open, we put the source code developed for this project publicly available at https://github.com/gomerudo/nas-dmrl. This development uses TensorFlow~\citep{tensorflow} as the framework for the building and training of the architectures, the OpenAI baselines~\citep{openaibaselines} for the reinforcement learning algorithm, and includes an OpenAI Gym~\citep{openaigym} environment that we developed and named \textsc{nasgym}, which can be used with any of the available OpenAI baselines' algorithms. We believe that this is an important contribution that will allow to study our framework with variations that are programmatically easy to implement, and it will inspire future research for NAS.

\subsection{Experiments}\label{sec:methodology:experiments}

\subsubsection{The baseline}\label{sec:methodology:experiments:baseline}

\textbf{TODO: Still we will need to define the parameters used for Q-learning.}
