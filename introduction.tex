\section{Introduction}

Neural networks have achieved remarkable results in many fields, such as that of Image Classification. Crucial aspects of this success are the choice of the neural architecture and the chosen hyperparameters for the particular dataset of interest; however, this is not always straightforward. Although state-of-the-art neural networks can inspire the design of other architectures, this process heavily relies on the designer's level of expertise, making it a challenging and cumbersome task that is prone to deliver underperforming networks.

In an attempt to overcome these flaws, researchers have explored various techniques under the name of Neural Architecture Search (NAS)~\citep{NASsurvey}. In NAS, the ultimate goal is to come up with an algorithm that takes any arbitrary dataset as input and outputs a well-performing neural network for some learning task of interest, so that we can accelerate the design process and remove the dependency on human intervention. Nevertheless, coming up with a solution of this kind is a complicated endeavor where researchers have to deal with several aspects such as the type of the networks that they consider, the scope of the automation process, or the search strategy applied. A particular search strategy for NAS is reinforcement learning (RL), where a so-called \textit{agent} learns how to design neural networks by sampling architectures and using their numeric performance on a specific dataset as the reward signals that guide the search. Popular standard RL algorithms such as \textsc{Q-learning} or \textsc{Reinforce} have been used to design state-of-the-art Convolutional Neural Networks (CNNs) for classification tasks on the CIFAR and ImageNet datasets \citep{ENAS, PathNAS, BlockQNN, ZophNAS1, BakerNAS}, but little attention is paid to deliver architectures for other datasets. In an attempt to fill that gap, a suitable alternative is deep meta-RL~\citep{LtRL, RL2}, where the \textit{agent} acts on various environments to learn an \textit{adaptive} policy that can be transferred to new environments.

In this work, we apply deep meta-RL to NAS, which, to the best of our knowledge, is a novel contribution. The environments that we consider are associated with standard image classification tasks on datasets with different levels of hardness sampled from a meta-dataset~\citep{MetaDataset}. Our main experiments focus on the design of chain-structured networks and show that, under resource constraints, the resulting policy can adapt to new environments, outperform standard RL, and design better architectures than the ones inspired by state-of-the-art networks. We also experiment with extending our approach to the design of multi-branch architectures so that we can give directions for future work.


The remainder of this report is structured as follows. First, in  Section ~\ref{sec:preliminaries}, we introduce the preliminary concepts required to understand our work. Next, in Section~\ref{sec:related}, we discuss the related work for both reinforcement learning and NAS. In Section~\ref{sec:methodology}, we formally introduce our methodology, and in Section~\ref{sec:software}, the framework developed to implement it. In Section~\ref{sec:experiments}, we define the experiments, and in Section~\ref{sec:results}, we show the results. Finally, in Section~\ref{sec:conclusions}, the conclusions are set out.